# rosaOS

ROS Agentic Operating System: Control robots with LLMs through MCP with Reachy Mini as the interface.

## Requirements
Using Reachy Mini Lite for easy media stream.

For LLM client, currently supports local endpoint or Groq API.
Defaults to local and if endpoint is not accessible, uses Groq.
Currently using gpt-oss-120b.

### Local LLM
For local setup, SSH into a GPU server and deploy with [vLLM](https://docs.vllm.ai/en/latest/getting_started/quickstart/):

`vllm serve openai/gpt-oss-120b --tool-call-parser openai --enable-auto-tool-choice --port 6000`

This is done in VS Code for the automatic port forwarding. To test this is successfully, run this locally and you should see the model:

`curl http://localhost:6000/v1/models`

This endpoint is currently hardcoded. Change in code if different.

### Groq API
[Groq](https://console.groq.com/keys) is an inference provider with a free tier for personal use but has limits.
To use it, get an API key and set it as an environment variable.

`GROQ_API_KEY=...`

Required for image analysis and better TTS experience. 

## Installation

Developed with Python 3.12.

```
git clone https://github.com/lilyjge/reachy-mcp.git
cd reachy-mcp
python -m venv reachy_mini_env
.\reachy_mini_env\Scripts\activate.ps1
pip install -r requirements.txt
```

## Usage
Start Reachy Mini's robot daemon server on the default port 8000:

`uv run reachy-mini-daemon`

Start the Reachy Mini's MCP server on port 5001:

`python -m server`

Start the operating system's client on port 8765:

`python -m client`

Now you can talk to the Reachy Mini directly.

If you want to directly chat with LLM without going through Reachy, you can launch a CLI chat:

`python -m client.chat.client_cli`

Or, when the agent is running, visit `http://localhost:8765/` in your browser.

### Configuration

Customize system prompts in `client\utils.py`.

Configure robot MCP servers in `client\config.json`.

## Technical Details

rosaOS is structured like a minimal **operating system**: a **kernel** schedules and supervises **processes** (LLM workers) that perform tasks, while a **device layer** (MCP server) exposes hardware (Reachy Mini) as callable tools. The LLM is the “CPU” that executes kernel and process logic.

### High-level architecture

| Layer | Component | OS analogy | Role |
|-------|-----------|------------|------|
| **User / shell** | Reachy Mini, or to chat directly, browser UI or CLI | Shell / terminal | Sends prompts and receives responses; polls for event-driven updates. |
| **Kernel** | Client event worker + Pydantic-AI “kernel” agent | OS kernel / scheduler | Single thread consumes an event queue (speech, worker callbacks, chat messages). Decides when to **launch processes** (workers) via the process server; does not drive the robot directly. |
| **Process manager** | Internal MCP server for kernel | Syscall interface / `fork` | Exposes process management tools to kernel. Spawns worker **subprocesses** (`python -m client.worker`) so each agent has its own event loop and does not block the kernel. |
| **Processes** | Agent worker subprocesses | User processes | Each runs a Pydantic-AI agent with **MCP robot tools**. Executes one task from a system prompt generated by kernel, then POSTs a completion **callback** to the client `/event`. |
| **Device layer** | Reachy MCP server, optionally easily connect additional robot MCP servers | Drivers / HAL | FastMCP server with lifespan owning the ReachyMini connection. Registers tools: `goto_target`, `take_picture`, `speak`, `play_emotion`, `describe_image`, etc. Runs a background **STT loop**: mic → VAD → transcribe → POST to client `/stt`, like a system process for the UI. |
| **Hardware** | Reachy Mini + other robot | Physical devices | Robot daemon and hardware; MCP server talks to Reachy via `reachy_mini` SDK and other robots through ROS. |

### Data flow 

1. **User input** → Speech via Reachy mic is transcribed by the server’s STT loop and POSTed to client `/stt`; or text is sent via CLI or the UI.
2. **Kernel** receives an event (`[User said] ...` or `[Worker callback] ...`). It runs the kernel agent (LLM) with tools from the **process server**, typically calling `launch_process(system_prompt)` to start a worker.
3. **Process manager** starts a worker subprocess with `WORKER_ID`, `WORKER_SYSTEM_PROMPT`, and `CALLBACK_URL` (client `/event`).
4. **Worker** runs the process agent (LLM) with tools from the **Reachy MCP server**: move, see, speak, etc. When done, it POSTs `{ worker_id, message, done }` to `/event`.
5. **Kernel** gets a `[Worker callback]` event and can respond to the user (e.g. via another launched process that uses `speak`) or launch further work. Primary communication to the user is through Reachy speaking; outgoing messages are also pushed to `/updates` for the UI/CLI to poll.

So: **kernel** = one agent that only launches processes; **processes** = short-lived agents that use the robot and report back via callbacks.

### Architecture diagram

See [docs/architecture.md](docs/architecture.md) for a diagram (Mermaid) of the same layout.